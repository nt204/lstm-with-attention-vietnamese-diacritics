{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8513128,"sourceType":"datasetVersion","datasetId":5082144},{"sourceId":12551249,"sourceType":"datasetVersion","datasetId":7924757}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport re\nimport string\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport pickle\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:19:46.563272Z","iopub.execute_input":"2025-08-07T11:19:46.563448Z","iopub.status.idle":"2025-08-07T11:20:16.447604Z","shell.execute_reply.started":"2025-08-07T11:19:46.563431Z","shell.execute_reply":"2025-08-07T11:20:16.447023Z"}},"outputs":[{"name":"stderr","text":"2025-08-07 11:19:58.195436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754565598.558658      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754565598.667925      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- DEVICE SETUP ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:20:16.448323Z","iopub.execute_input":"2025-08-07T11:20:16.448772Z","iopub.status.idle":"2025-08-07T11:20:16.540183Z","shell.execute_reply.started":"2025-08-07T11:20:16.448738Z","shell.execute_reply":"2025-08-07T11:20:16.539487Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --- CLEANING FUNCTION ---\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# --- LOAD DATA ---\ndf = pd.read_csv(\"/kaggle/input/vietnamese-sentences/sentences.csv\")\ndf = df.dropna(subset=[\"sentence_with_diacritics\", \"sentence_without_diacritics\"]).reset_index(drop=True)\n\n# --- SHUFFLE ---\ndf = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\n# --- SPLIT ---\nn = len(df)\nn_train = int(0.8 * n)\nn_val   = int(0.1 * n)\nn_test  = n - n_train - n_val\n\ndf_train = df[:n_train].copy()\ndf_val   = df[n_train:n_train + n_val].copy()\ndf_test  = df[n_train + n_val:].copy()\n\n# --- CLEAN + TOKENIZE ---\nfor df_ in [df_train, df_val, df_test]:\n    df_['no_diacritics_clean']   = df_['sentence_without_diacritics'].astype(str).apply(clean_text)\n    df_['with_diacritics_clean'] = df_['sentence_with_diacritics'].astype(str).apply(clean_text)\n    df_['with_diacritics_clean'] = df_['with_diacritics_clean'].apply(lambda x: '<start> ' + x + ' <end>')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:20:16.542310Z","iopub.execute_input":"2025-08-07T11:20:16.542910Z","iopub.status.idle":"2025-08-07T11:20:47.958185Z","shell.execute_reply.started":"2025-08-07T11:20:16.542880Z","shell.execute_reply":"2025-08-07T11:20:47.957545Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- TOKENIZATION ---\nfilters = '\"!#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'  # preserve <> for start/end\nsrc_tokenizer = Tokenizer(oov_token='<unk>', filters=filters)\ntgt_tokenizer = Tokenizer(oov_token='<unk>', filters=filters)\nsrc_tokenizer.fit_on_texts(df_train['no_diacritics_clean'])\ntgt_tokenizer.fit_on_texts(df_train['with_diacritics_clean'])\n\nSRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\nTGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\nMAX_LEN = 70\n\nwith open(\"tokenized.pkl\", \"wb\") as f:\n    pickle.dump({\n        \"src_tokenizer\": src_tokenizer,\n        \"tgt_tokenizer\": tgt_tokenizer,\n        \"SRC_VOCAB_SIZE\": SRC_VOCAB_SIZE,\n        \"TGT_VOCAB_SIZE\": TGT_VOCAB_SIZE,\n        \"MAX_LEN\": MAX_LEN\n    }, f)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:20:47.958840Z","iopub.execute_input":"2025-08-07T11:20:47.959049Z","iopub.status.idle":"2025-08-07T11:21:18.239515Z","shell.execute_reply.started":"2025-08-07T11:20:47.959031Z","shell.execute_reply":"2025-08-07T11:21:18.238913Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# encode & pad\ndef encode_and_pad(texts, tokenizer):\n    seqs = tokenizer.texts_to_sequences(texts)\n    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post')\n\ntrain_src = encode_and_pad(df_train['no_diacritics_clean'], src_tokenizer)\ntrain_tgt = encode_and_pad(df_train['with_diacritics_clean'], tgt_tokenizer)\nval_src   = encode_and_pad(df_val['no_diacritics_clean'], src_tokenizer)\nval_tgt   = encode_and_pad(df_val['with_diacritics_clean'], tgt_tokenizer)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:18.240174Z","iopub.execute_input":"2025-08-07T11:21:18.240397Z","iopub.status.idle":"2025-08-07T11:21:50.031529Z","shell.execute_reply.started":"2025-08-07T11:21:18.240379Z","shell.execute_reply":"2025-08-07T11:21:50.030937Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Câu gốc không dấu và có dấu\nno_diacritic_sentence = df_train['no_diacritics_clean'].iloc[0]\nwith_diacritic_sentence = df_train['with_diacritics_clean'].iloc[0]\n\n# Tokenized & padded\nsrc_tokens = train_src[0]\ntgt_tokens = train_tgt[0]\n\nprint(\"Câu không dấu     :\", no_diacritic_sentence)\nprint(\"Câu có dấu        :\", with_diacritic_sentence)\nprint(\"Token hóa (src)   :\", src_tokens)\nprint(\"Token hóa (tgt)   :\", tgt_tokens)\n\n# Nếu muốn xem lại dạng từ (decode ngược)\nprint(\"\\nGiải mã lại từ token:\")\nprint(\"src decode:\", ' '.join(src_tokenizer.sequences_to_texts([src_tokens])[0].split()))\nprint(\"tgt decode:\", ' '.join(tgt_tokenizer.sequences_to_texts([tgt_tokens])[0].split()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.032309Z","iopub.execute_input":"2025-08-07T11:21:50.032561Z","iopub.status.idle":"2025-08-07T11:21:50.039190Z","shell.execute_reply.started":"2025-08-07T11:21:50.032540Z","shell.execute_reply":"2025-08-07T11:21:50.038510Z"}},"outputs":[{"name":"stdout","text":"Câu không dấu     : theo danh gia cua gioi chuyen mon nhieu kha nang viet nam se co them it nhat hai chiec hc vang nua\nCâu có dấu        : <start> theo đánh giá của giới chuyên môn nhiều khả năng việt nam sẽ có thêm ít nhất hai chiếc hc vàng nữa <end>\nToken hóa (src)   : [ 92 146  32   3 211 104 328  67 280 135 140  27  50   2 309 381  89  74\n 397 859 362 314   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\nToken hóa (tgt)   : [   2   58  280   89    4  176  355  471   40  459  225  120   80   24\n    6  297  391   73   85  423 1569  515  377    3    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n\nGiải mã lại từ token:\nsrc decode: theo danh gia cua gioi chuyen mon nhieu kha nang viet nam se co them it nhat hai chiec hc vang nua <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\ntgt decode: <start> theo đánh giá của giới chuyên môn nhiều khả năng việt nam sẽ có thêm ít nhất hai chiếc hc vàng nữa <end> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- DATASET & LOADER ---\nclass TranslationDataset(Dataset):\n    def __init__(self, src, tgt):\n        self.src = torch.LongTensor(src)\n        self.tgt = torch.LongTensor(tgt)\n    def __len__(self):\n        return len(self.src)\n    def __getitem__(self, idx):\n        return self.src[idx], self.tgt[idx]\n\nBATCH_SIZE = 64\ntrain_loader = DataLoader(TranslationDataset(train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(TranslationDataset(val_src, val_tgt), batch_size=BATCH_SIZE)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.039902Z","iopub.execute_input":"2025-08-07T11:21:50.040223Z","iopub.status.idle":"2025-08-07T11:21:50.400809Z","shell.execute_reply.started":"2025-08-07T11:21:50.040194Z","shell.execute_reply":"2025-08-07T11:21:50.400002Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- SCRATCH LSTM MODULE ---\nclass LSTMScratch(nn.Module):\n    def __init__(self, input_size, hidden_size, sigma=0.01):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        def init_weight(*shape):\n            return nn.Parameter(torch.randn(*shape) * sigma)\n\n        def init_gate():\n            return (\n                init_weight(input_size, hidden_size),\n                init_weight(hidden_size, hidden_size),\n                nn.Parameter(torch.zeros(hidden_size))\n            )\n\n        # Input gate\n        self.W_xi, self.W_hi, self.b_i = init_gate()\n        # Forget gate\n        self.W_xf, self.W_hf, self.b_f = init_gate()\n        # Output gate\n        self.W_xo, self.W_ho, self.b_o = init_gate()\n        # Candidate cell state\n        self.W_xc, self.W_hc, self.b_c = init_gate()\n\n    def forward(self, X, H, C):\n        I = torch.sigmoid(X @ self.W_xi + H @ self.W_hi + self.b_i)\n        F = torch.sigmoid(X @ self.W_xf + H @ self.W_hf + self.b_f)\n        O = torch.sigmoid(X @ self.W_xo + H @ self.W_ho + self.b_o)\n        C_tilde = torch.tanh(X @ self.W_xc + H @ self.W_hc + self.b_c)\n        C_next = F * C + I * C_tilde\n        H_next = O * torch.tanh(C_next)\n        return H_next, C_next\n\n# --- ATTENTION MODULE ---\nclass LuongAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden: (batch, hidden)\n        # encoder_outputs: (seq_len, batch, hidden)\n        seq_len, batch_size, hidden = encoder_outputs.size()\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # (batch, seq_len, hidden)\n        decoder_hidden = decoder_hidden.unsqueeze(1)        # (batch, 1, hidden)\n        # Score = h_t^T W h_s\n        energy = torch.bmm(decoder_hidden, self.linear(encoder_outputs).transpose(1, 2))  # (batch, 1, seq_len)\n        attn_weights = torch.softmax(energy, dim=-1)  # (batch, 1, seq_len)\n        context = torch.bmm(attn_weights, encoder_outputs)  # (batch, 1, hidden)\n        return context.squeeze(1), attn_weights.squeeze(1)  # (batch, hidden), (batch, seq_len)\n\n# --- ENCODER WITH LSTM ---\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = LSTMScratch(embed_size, hidden_size)\n\n    def forward(self, x):\n        embeddings = self.embedding(x)\n        H = torch.zeros(x.shape[1], self.lstm.hidden_size, device=x.device)\n        C = torch.zeros_like(H)\n        outputs = []\n        for emb in embeddings:\n            H, C = self.lstm(emb, H, C)\n            outputs.append(H.unsqueeze(0))\n        return torch.cat(outputs, dim=0), (H, C)\n\n# --- DECODER WITH LSTM AND ATTENTION ---\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.attn = LuongAttention(hidden_size)\n        self.lstm = LSTMScratch(embed_size + hidden_size, hidden_size)\n        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n\n    def forward(self, x, state, encoder_outputs):\n        H, C = state\n        embeddings = self.embedding(x)\n        outputs = []\n        for emb in embeddings:\n            context, _ = self.attn(H, encoder_outputs)\n            lstm_input = torch.cat([emb, context], dim=1)\n            H, C = self.lstm(lstm_input, H, C)\n            out = self.fc(torch.cat([H, context], dim=1))\n            outputs.append(out.unsqueeze(0))\n        return torch.cat(outputs, dim=0), (H, C)\n\n# --- SEQ2SEQ ---\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, tgt):\n        src = src.transpose(0, 1)\n        tgt = tgt.transpose(0, 1)\n        enc_outputs, (hidden, cell) = self.encoder(src)\n        outputs, _ = self.decoder(tgt, (hidden, cell), enc_outputs)\n        return outputs.transpose(0, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.401888Z","iopub.execute_input":"2025-08-07T11:21:50.402185Z","iopub.status.idle":"2025-08-07T11:21:50.416115Z","shell.execute_reply.started":"2025-08-07T11:21:50.402160Z","shell.execute_reply":"2025-08-07T11:21:50.415525Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nimport csv\nimport os\n\n# --- MODEL INIT ---\nEMBED_SIZE = 256\nHIDDEN_SIZE = 512\nencoder = Encoder(SRC_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\ndecoder = Decoder(TGT_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\nmodel = Seq2Seq(encoder, decoder).to(device)\n\n# --- LOAD BEST CHECKPOINT ---\nmodel.load_state_dict(torch.load(\"/kaggle/input/pre-model/best_model.pt\"))\nmodel.train()  \n\n# --- TRAINING SETUP ---\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\nscaler = GradScaler()\n\n# --- LOG & CALLBACK SETUP ---\nlog_history = []\nbest_val_loss = float('inf')  # Nếu bạn nhớ giá trị epoch 5 thì gán lại\npatience = 5\ncounter = 0\n\n# --- FILE LOG ---\nlog_file = \"training_log.csv\"\nwrite_header = not os.path.exists(log_file)\nif write_header:\n    with open(log_file, \"w\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=[\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"])\n        writer.writeheader()\n\n# --- TRAIN LOOP ---\nEPOCHS = 50\nSTART_EPOCH = 1 \n\nfor epoch in range(START_EPOCH, EPOCHS):\n    model.train()criterion\n    total_loss = total_tokens = total_correct = 0\n\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n    for src, tgt in progress_bar:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        with autocast():\n            logits = model(src, tgt_input)  # (batch, tgt_len-1, vocab)\n            logits = logits.reshape(-1, TGT_VOCAB_SIZE)\n            tgt_flat = tgt_output.reshape(-1)\n            loss = criterion(logits, tgt_flat)\n\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        # metrics\n        mask = tgt_flat != 0\n        total_loss += loss.item() * mask.sum().item()\n        total_tokens += mask.sum().item()\n        preds = logits.argmax(dim=1)\n        total_correct += (preds == tgt_flat).masked_select(mask).sum().item()\n\n        batch_loss = loss.item()\n        batch_acc = (preds == tgt_flat).masked_select(mask).float().mean().item()\n        progress_bar.set_postfix({\n            \"Batch Loss\": f\"{batch_loss:.4f}\",\n            \"Batch Acc\": f\"{batch_acc:.4f}\"\n        })\n\n    scheduler.step()\n    train_loss = total_loss / total_tokens\n    train_acc = total_correct / total_tokens\n    print(f\"\\n📘 Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n\n    # --- VALIDATION ---\n    model.eval()\n    val_loss = val_tokens = val_correct = 0\n\n    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n    with torch.no_grad():\n        for src, tgt in val_bar:\n            src, tgt = src.to(device), tgt.to(device)\n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n            logits = model(src, tgt_input).reshape(-1, TGT_VOCAB_SIZE)\n            tgt_flat = tgt_output.reshape(-1)\n            loss = criterion(logits, tgt_flat)\n\n            mask = tgt_flat != 0\n            val_loss += loss.item() * mask.sum().item()\n            val_tokens += mask.sum().item()\n            preds = logits.argmax(dim=1)\n            val_correct += (preds == tgt_flat).masked_select(mask).sum().item()\n\n            val_batch_loss = loss.item()\n            val_batch_acc = (preds == tgt_flat).masked_select(mask).float().mean().item()\n            val_bar.set_postfix({\n                \"Val Loss\": f\"{val_batch_loss:.4f}\",\n                \"Val Acc\": f\"{val_batch_acc:.4f}\"\n            })\n\n    val_loss_avg = val_loss / val_tokens\n    val_acc_avg = val_correct / val_tokens\n    print(f\"✅ [Val] Loss: {val_loss_avg:.4f} | Acc: {val_acc_avg:.4f}\")\n\n    # --- LOGGING ---\n    log_record = {\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"train_acc\": train_acc,\n        \"val_loss\": val_loss_avg,\n        \"val_acc\": val_acc_avg\n    }\n    log_history.append(log_record)\n\n    # Ghi log từng epoch vào CSV ngay\n    with open(log_file, \"a\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=[\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"])\n        writer.writerow(log_record)\n\n    # --- CALLBACK: EARLY STOPPING & CHECKPOINT ---\n    if val_loss_avg < best_val_loss:\n        best_val_loss = val_loss_avg\n        counter = 0\n        torch.save(model.state_dict(), 'best_model.pt')\n        print(\"💾 Model improved. Checkpoint saved.\")\n    else:\n        counter += 1\n        print(f\"⏳ No improvement. EarlyStop counter: {counter}/{patience}\")\n        if counter >= patience:\n            print(\"🛑 Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.418559Z","iopub.execute_input":"2025-08-07T11:21:50.419286Z","iopub.status.idle":"2025-08-07T11:21:50.441746Z","shell.execute_reply.started":"2025-08-07T11:21:50.419264Z","shell.execute_reply":"2025-08-07T11:21:50.440799Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35/1382961625.py\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    model.train()criterion\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (1382961625.py, line 45)","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# --- Khởi tạo model ---\nEMBED_SIZE = 256\nHIDDEN_SIZE = 512\nencoder = Encoder(SRC_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\ndecoder = Decoder(TGT_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\nmodel = Seq2Seq(encoder, decoder).to(device)\n\n# --- Load checkpoint tốt nhất ---\nmodel.load_state_dict(torch.load(\"/kaggle/input/version1/best_model.pt\"))\nmodel.eval() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.442379Z","iopub.status.idle":"2025-08-07T11:21:50.442655Z","shell.execute_reply.started":"2025-08-07T11:21:50.442507Z","shell.execute_reply":"2025-08-07T11:21:50.442525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_src = encode_and_pad(df_test['no_diacritics_clean'], src_tokenizer)\ntest_tgt = encode_and_pad(df_test['with_diacritics_clean'], tgt_tokenizer)\n\ntest_loader = DataLoader(TranslationDataset(test_src, test_tgt), batch_size=32)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # bỏ qua <pad> token\ntotal_loss = 0.0\ntotal_correct = 0\ntotal_tokens = 0\n\nwith torch.no_grad():\n    for src, tgt in tqdm(test_loader, desc=\"Evaluating on test set\"):\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        logits = model(src, tgt_input)\n        logits = logits.reshape(-1, logits.shape[-1])\n        tgt_flat = tgt_output.reshape(-1)\n\n        loss = criterion(logits, tgt_flat)\n        total_loss += loss.item()\n\n        preds = torch.argmax(logits, dim=1)\n        mask = tgt_flat != 0\n        correct = (preds == tgt_flat) & mask\n        total_correct += correct.sum().item()\n        total_tokens += mask.sum().item()\n\navg_loss = total_loss / len(test_loader)\naccuracy = total_correct / total_tokens\n\nprint(f\"Test Loss: {avg_loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.443653Z","iopub.status.idle":"2025-08-07T11:21:50.444008Z","shell.execute_reply.started":"2025-08-07T11:21:50.443836Z","shell.execute_reply":"2025-08-07T11:21:50.443853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2word = {idx: word for word, idx in tgt_tokenizer.word_index.items()}\nidx2word[0] = '<pad>'\n\ndef beam_search_decode(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, beam_width=3, max_len=70):\n    model.eval()\n\n    # --- Clean and encode input ---\n    cleaned_input = input_sentence.strip().lower()\n    input_seq = pad_sequences(src_tokenizer.texts_to_sequences([cleaned_input]), maxlen=max_len, padding='post')\n    input_tensor = torch.LongTensor(input_seq).to(device)\n\n    start_token = tgt_tokenizer.word_index.get('<start>', 1)\n    end_token = tgt_tokenizer.word_index.get('<end>', 2)\n\n    sequences = [[start_token]]\n    scores = [0.0]\n    completed_sequences = []\n\n    for _ in range(max_len):\n        all_candidates = []\n        for seq, score in zip(sequences, scores):\n            if seq[-1] == end_token:\n                completed_sequences.append((seq, score))\n                continue\n\n            tgt_tensor = torch.LongTensor([seq]).to(device)\n            with torch.no_grad():\n                output = model(input_tensor, tgt_tensor)\n                logits = output[0, -1, :]\n                log_probs = torch.log_softmax(logits, dim=-1)\n\n            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n            for j in range(beam_width):\n                next_token = topk_indices[j].item()\n                next_score = score + topk_log_probs[j].item()\n                all_candidates.append((seq + [next_token], next_score))\n\n        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n        sequences = [seq for seq, _ in ordered[:beam_width]]\n        scores = [score for _, score in ordered[:beam_width]]\n\n        if all(seq[-1] == end_token for seq in sequences):\n            break\n\n    if completed_sequences:\n        best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n    else:\n        best_seq = sequences[0]\n\n    decoded = []\n    for token in best_seq[1:]:\n        if token == end_token:\n            break\n        decoded.append(idx2word.get(token, '<unk>'))\n\n    return ' '.join(decoded)\n\n# --- Example test ---\ntest_sentences = [\n    \"toi yeu tieng viet\",\n    \"chung ta se chien thang\",\n    \"ha noi la thu do cua viet nam\"\n]\n\nprint(\"\\nKết quả dự đoán:\")\nfor sent in test_sentences:\n    print(\"Input:\", sent)\n    print(\"Output:\", beam_search_decode(model, sent, src_tokenizer, tgt_tokenizer, {v:k for k,v in tgt_tokenizer.word_index.items()}, beam_width=5))\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.445576Z","iopub.status.idle":"2025-08-07T11:21:50.446124Z","shell.execute_reply.started":"2025-08-07T11:21:50.445945Z","shell.execute_reply":"2025-08-07T11:21:50.445961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.446969Z","iopub.status.idle":"2025-08-07T11:21:50.447209Z","shell.execute_reply.started":"2025-08-07T11:21:50.447077Z","shell.execute_reply":"2025-08-07T11:21:50.447086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Advanced BLEU Evaluation Function (Optimized and Precise) ---\nimport sacrebleu\nfrom tqdm import tqdm\nimport torch\n\ndef evaluate_bleu(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n    model.eval()\n    references = []\n    hypotheses = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating BLEU\"):\n        input_sentence = row['no_diacritics_clean']\n        reference = row['with_diacritics_clean']\n\n        # Clean target (remove <start> and <end>)\n        reference = reference.replace('<start>', '').replace('<end>', '').strip()\n\n        # Decode prediction from model\n        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n        prediction = prediction.strip()\n\n        references.append([reference])\n        hypotheses.append(prediction)\n\n    # Compute BLEU\n    bleu = sacrebleu.corpus_bleu(hypotheses, list(map(list, zip(*references))))\n    print(f\"\\nFinal BLEU Score: {bleu.score:.2f}\")\n    return bleu.score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.448431Z","iopub.status.idle":"2025-08-07T11:21:50.448633Z","shell.execute_reply.started":"2025-08-07T11:21:50.448536Z","shell.execute_reply":"2025-08-07T11:21:50.448545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# score = evaluate_bleu(\n#     model,\n#     df_test.sample(n=5000, random_state=42).reset_index(drop=True),\n#     src_tokenizer,\n#     tgt_tokenizer,\n#     idx2word,\n#     decode_fn=beam_search_decode\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:21:50.449793Z","iopub.status.idle":"2025-08-07T11:21:50.450010Z","shell.execute_reply.started":"2025-08-07T11:21:50.449914Z","shell.execute_reply":"2025-08-07T11:21:50.449923Z"}},"outputs":[],"execution_count":null}]}